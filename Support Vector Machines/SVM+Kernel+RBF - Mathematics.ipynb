{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "1. Suppose we have linearly seperable data. There could be many lines which seperates the data. Is there any advantage of choosing one line over the other?\n",
    "\n",
    "2. In SVM classifier we think of a line with some margin seperating the data. The best line seperating the data would be the line with the biggest margin.\n",
    "\n",
    "3. **Why bigger margin is better?** : There is less chances of missclassification on unseen data, that are near to the line seperating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding w with large margin.\n",
    "\n",
    "1. Let $x_n$ be the nearest data point to the line (hyperplane) with equation $ w^T.x = 0 $. ($x_n$ are called support vectors)\n",
    "\n",
    "2. 2 prelimary techicalities\n",
    "\n",
    "    > i. For nearest point to the hyperplane. $ | w^T.x_n | = 1 $. As we away from hyperplane the absolute value increase.\n",
    "    \n",
    "    > ii. Pull out $w_0$. So w becomes $(w_1, w_2, w_3, w_4 ... w_d)$ apart from b.\n",
    "      The plane is now $ w^T.x + b = 0 $. There is no $x_0$. This is just a convention.\n",
    "\n",
    "3. The vector 2 is perpendicular to hyperplane ($ w^T.x + b =0 $) in x space.\n",
    "    \n",
    "    > Proof. Take x' and x'' on the plane. $w^T.x' + b= 0$ and $ w^T.x'' + b = 0 $\n",
    "    \n",
    "    > This gives $w^T.(x' - x'') = 0$, which means w is perpendicular to hyperplane.\n",
    "\n",
    "4. The distance between $x_n$ and the hyperplane (margin):\n",
    "    > Take any point x on the hyperplane. Projection of $x_n-x$ on w will be the distance.\n",
    "    \n",
    "    > $\\hat w = \\frac{w}{||w||} $\n",
    "    \n",
    "    > $ \\text{distance} = | \\hat w^T.(x_n - x) | = \\frac{1}{||w||}| w^T.x_n + b - w^T.x - b | = \\frac{1}{||w||}| 1 - 0 | = \\frac{1}{||w||}$ \n",
    "    \n",
    "5. The Optimization Problem\n",
    "\n",
    "    > Maximize $\\frac{1}{||w||}$, subject to $ \\min_{n=1,2,..N} |w^T.x_n + b| = 1 $\n",
    "    \n",
    "    > Satifying min contraint is not a nice problem, We could convert it into another problem by simple observation. Notice $ |w^T.x_n + b| = y_n.(w^T.x+n + b) $, where y_n could be -1 or 1.\n",
    "    \n",
    "    > Minimize $ \\frac{1}{2} w^T.w $, subject to $y_n.(w^T.x_n + b) >= 1$ for n = 1,2,...,N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Constrained optimization\n",
    "\n",
    "$$ \\text{Minimize : }  \\frac{1}{2}w^Tw  $$\n",
    "\n",
    "$$ \\text{subject to : } y_n(w^Tx_n+b) \\ge 1 \\text{ for n=1,2,...,N} $$\n",
    "\n",
    "**Constrained Optimization - Use Lagrange with Karush-Kuhn-Tucker Condition (for inequality constriant)**\n",
    "\n",
    "$$ \\text{Minimize} : L(w,b,\\alpha) = \\frac{1}{2}w^Tw - \\sum_{n=1}^N \\alpha_n\\big(y_n(w^Tx_n + b)-1\\big)$$\n",
    "\n",
    "w.r.t w and b and maximizing w.r.t each $\\alpha_n \\ge 0$\n",
    "\n",
    "\n",
    "$$ \\triangledown_w L = w - \\sum_{n=1}^N \\alpha_b y_nx_n  = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b} = - \\sum_{n=1}^N \\alpha_ny_n = 0 $$\n",
    "\n",
    "substituing in Lagrange\n",
    "\n",
    "$$ L = \\frac{1}{2}w^Tw + \\sum_{n=1}^N \\alpha_n + \\sum_{n=1}^N \\alpha_ny_n - \\sum_{n=1}^N \\alpha_ny_nb - \\sum_{n=1}^N \\alpha_ny_nw^Tx_n$$\n",
    "\n",
    "$$ L = \\frac{1}{2}w^Tw + \\sum_{n=1}^N \\alpha_n + 0 - 0 - w^Tw$$\n",
    "\n",
    "$$ L = \\sum_{n=1}^N \\alpha_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N y_ny_m\\alpha_n\\alpha_Mx_n^Tx_m $$\n",
    "\n",
    "Maximize w.r.t. $\\alpha$ subject to $\\alpha_n$ >= 0 for n = 1....N and $\\sum_{n=1}^N \\alpha_n y_n =0$\n",
    "\n",
    "**Solution using Quadratic Programming**\n",
    "\n",
    "$$ min_\\alpha  \\frac{1}{2} \\alpha^T\n",
    "\\begin{bmatrix}\n",
    "  y_1y_1x_1^Tx_1&y_1y_2x_1^Tx_2&\\cdots&y_1y_Nx_1^Tx_N\\\\\n",
    "  y_2y_1x_2^Tx_1&y_2y_2x_2^Tx_2&\\cdots&y_2y_Nx_2^Tx_N\\\\\n",
    "  \\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "  y_Ny_1x_N^Tx_1&y_Ny_2x_N^Tx_2&\\cdots&y_Ny_Nx_N^Tx_N\\\\\n",
    "\\end{bmatrix}\n",
    "\\alpha + (-1^T)\\alpha, \\text{ subject to } y^T\\alpha = 0 $$\n",
    "\n",
    "Now, get $$ w = \\sum_{n=1}^N \\alpha_ny_nx_n $$\n",
    "\n",
    "* Suppose we get N alphas, you find out of them 90% are 0 coz for non-support vectors slack $y_n(w^Tx_n+b)-1 \\ne 0 $ and to satisfy the equation $ \\alpha_n\\big(y_n(w^Tx_n + b)-1\\big) = 0$, there $\\alpha$ is 0.\n",
    "* Less number of support vector means solution is more generalized.\n",
    "* This was for linearly seperable data.\n",
    "* This is for Hard-Margin SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagrange Contrained Optimization (Equality Contraint)\n",
    "\n",
    "**Theorem** : Let f(x,y) be subject to constraint g(x,y) =0 and have a rel. min|max at (a,b). Then there exists $\\lambda$ (constant) such that the partials of below new function fn\n",
    "\n",
    "fn : $ F(x,y,\\lambda) = f(x,y) + \\lambda g(x,y) $, all equals zero at point (a,b,c).\n",
    "\n",
    "$ \\frac{\\partial F}{\\partial x} = \\frac{\\partial F}{\\partial y} = \\frac{\\partial F}{\\partial \\lambda} = 0 $\n",
    "\n",
    "\n",
    "#### Karush-Kuhn-Tucker Condition (Inequality Contraint) \n",
    "\n",
    "* Allowing for inquality constraints\n",
    "* Allowing for any number of contraints\n",
    "* Constraints may be binding or not at the solution\n",
    "* Allowing for non-negativity contraints (xi's >= 0)\n",
    "* Allowing for boundary solutions\n",
    "* Dual variables (Lagrange multipliers)\n",
    "\n",
    "Problem : Maximize f(x) subject to x >=0 and G(x) <= b. f is continuously differnetiable.\n",
    "\n",
    "To Prove : KKT is sufficient to get $\\hat x$ as a solution to problem.\n",
    "\n",
    "\n",
    "\n",
    "#### Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Non-Linear seperable data:\n",
    "\n",
    "1. We transform the data from X space into Z space (can have upto infinite dimensions) using non-linear transformation.\n",
    "\n",
    "2. Lets assume, in Z space the data is linearly seperable, we will identify the support vectors, and the margin.\n",
    "\n",
    "3. $$ L(\\alpha) = \\sum_{n=1}^N \\alpha_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N y_ny_m\\alpha_n\\alpha_Mz_n^Tz_m $$\n",
    "\n",
    "4. The points in Z space which are support vectors are also the support vectors in X space. (In X space there are usually called \"pre-images\" of support vectors.)\n",
    "\n",
    "5. **Lesser the support vectors better the generalization**. Think of a line going around every point in Z space which will have many support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Methods (Kernel Trick)\n",
    "\n",
    "1. If you want to go in higher dimensions without paying the price fot it. In way z space manifests itself in computation is simple, just we take inner product of in z space. Dimensionality of problem depends on the number of examples not on the no of dimensions.\n",
    "2. We have this Lagrange to solve : $ L(\\alpha) = \\sum_{n=1}^N \\alpha_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N y_ny_m\\alpha_n\\alpha_Mz_n^Tz_m $, where $\\alpha_n \\ge 0 $ and $\\sum_{n=1}^N \\alpha_ny_n=0$\n",
    "3. For hypothesis we need: $ g(x) = sign(w^Tz+b) \\text{ where } w = \\sum_{z_n in SV} \\alpha_ny_nz_n $.So for everything, we just need inner product in z space. So I can work with this machinery, if I can get an inner product without visiting z space.\n",
    "4. Given two points x and x', we need $z^Tz'$, Let $ z^Tz' = K(x,x') $ (the kernel)\n",
    "\n",
    "        Example x = (x1, x2)\n",
    "        \n",
    "        z = phi(x) = (1, x1, x2, x1^2, x2^2, x1.x2)\n",
    "        \n",
    "        K(x,x') = z^Tz' = 1 + x1.x'1 + x2.x'2 + x1^2.x'1^2 + x2^2.x'2^2 + x1.x'1.x2.x'2\n",
    "        \n",
    "5. **The Trick :** Can we compute this kernel without transforming x and x'?\n",
    "        \n",
    "        Example: Consider K(x,x') = (1+x^T.x')^2 = (1 + x1.x'1 + x2.x'2)^2\n",
    "                                  = 1 + x1^2.x'1^2 + x2^2.x'2^2+2.x1.x'1+2.x2.x'2+2.x1.x'1.x2.x'2\n",
    "         \n",
    "         This is an inner product of (x,x')\n",
    "         \n",
    "         x  = ( 1, x1^2, x2^2, sqrt(2).x1, sqrt(2).x2, sqrt(2).x1.x2 )\n",
    "         x' = ( 1, x'1^2, x'2^2, sqrt(2).x'1, sqrt(2).x'2, sqrt(2).x'1.x'2 )\n",
    "\n",
    "6. **The Polynomial Kernel**: X = $R^d$ and $ \\phi : X \\rightarrow Z \\text{ is polynomial of order Q } $\n",
    "\n",
    "    $$ \\text{Kernel } K(x,x') = (b + ax^Tx')^Q$$\n",
    "    \n",
    "7. **The Radial Basis Function (RBF Kernel)**: This maps X space to infinite dimensional Z space.\n",
    "\n",
    "    $$ \\text{Kernel } K(x,x') = exp( -\\gamma || x-x' ||^2 ) $$\n",
    "    \n",
    "    **Proof**: Infinite-dimensional Z\n",
    "    \n",
    "    * Suppose x and x' are scaler (1 dimensional) and $\\gamma = 1$\n",
    "    * Let using tailor series on expanded kernel\n",
    "        \n",
    "        $ K(x,x') = exp( -(x-x')^2 ) = exp( -x^2 - x'^2 +2.x.x' ) $\n",
    "        \n",
    "        $ = exp(-x^2).exp(-x'^2).exp(2.x.x') $\n",
    "        \n",
    "        $ = exp(-x^2).exp(-x'^2) \\sum_{k=0}^{\\infty} \\frac{2^k(x)^k(x')^k}{k!} \\text{ , using tailor expansion}$\n",
    "        \n",
    "8. Quadratic Programming and hypothesis using kernel\n",
    "\n",
    "    $ min_\\alpha  \\frac{1}{2} \\alpha^T\n",
    "\\begin{bmatrix}\n",
    "  y_1y_1K(x_1,x_1)&y_1y_2K(x_1,x_2)&\\cdots&y_1y_NK(x_1,x_N)\\\\\n",
    "  y_2y_1K(x_2,x_1)&y_2y_2K(x_2,x_2)&\\cdots&y_2y_NK(x_2,x_N)\\\\\n",
    "  \\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "  y_Ny_1K(x_N,x_1)&y_Ny_2K(x_N,x_2)&\\cdots&y_Ny_NK(x_N,x_N)\\\\\n",
    "\\end{bmatrix}\n",
    "\\alpha + (-1^T)\\alpha, \\text{ subject to } y^T\\alpha = 0 $\n",
    "\n",
    "    **Final Hypothesis**\n",
    "    \n",
    "    $$ g(x) = sign(w^Tz+b) = sign\\bigg(\\sum_{\\alpha_n>0} \\alpha_ny_nK(x_n,x) + b \\bigg) $$\n",
    "    \n",
    "    $$ b = y_m - \\sum_{\\alpha_n > 0} \\alpha_ny_nK(x_n,x_m) \\text{ , for any support vector ($\\alpha_m$ > 0) } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft-Margin SVM\n",
    "\n",
    "1. Margin violation : $y_n(w^Tx_n + b) \\ge 1$ fails\n",
    "\n",
    "\n",
    "2. Qualtify : $y_n(w^Tx_N + b) \\ge 1 - \\epsilon_n \\text{ , $\\epsilon_n \\ge 0 $ } $\n",
    "\n",
    "\n",
    "3. Total Violation = $\\sum_{n=1}^N \\epsilon_n$\n",
    "\n",
    "\n",
    "4. New Optimization : $ \\frac{1}{2}w^Tw + C\\sum_{n=1}^N \\epsilon_n $\n",
    "\n",
    "\n",
    "5. If C (constant) is higher then we will allow very less violation.\n",
    "\n",
    "\n",
    "6. Optimation will be same but with $ 0 \\le \\alpha_n \\le C $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF)\n",
    "\n",
    "* Each $(x_n, y_n) \\in D$ influences h(x) based on $||x-x_n||$ (distance). In other words nearby points will have more effect on hypothesis.\n",
    "\n",
    "\n",
    "* Standard Form:\n",
    "  \n",
    "  $h(x) = \\sum_{n=1}^N w_n.exp\\big(-\\gamma ||x-x_n||^2\\big)$\n",
    "  \n",
    "  It is called radial because of this $||x-x_n||$\n",
    "  \n",
    "  $exp\\big(-\\gamma||x-x_n||^2\\big)$ is basis function (building block). It could be anything.\n",
    "\n",
    "\n",
    "* **The Learning Algorithm**\n",
    "\n",
    "    Finding w1,....,wN: to get h(x), based on D = (x1,y1), ..., (xN,yN)\n",
    "    \n",
    "    (In sample Error) $E_{in} = 0$: $h(x_n) = y_n$ for n = 1, ..., N\n",
    "    \n",
    "    **The Solution**\n",
    "    \n",
    "    $ \\sum_{m=1}^N w_m exp\\big(-\\gamma ||x_n - x_m||^2\\big) = y_n  $\n",
    "    \n",
    "    Total n equations, N equations in N unknowns\n",
    "    \n",
    "    $\\implies \\begin{bmatrix}\n",
    "  exp(-\\gamma||x_1-x_1||^2)&\\cdots&exp(-\\gamma||x_1-x_N||^2)\\\\\n",
    "  exp(-\\gamma||x_2-x_1||^2)&\\cdots&exp(-\\gamma||x_2-x_N||^2)\\\\\n",
    "  \\vdots&\\vdots&\\vdots\\\\\n",
    "  exp(-\\gamma||x_N-x_1||^2)&\\cdots&exp(-\\gamma||x_N-x_N||^2)\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}w_1\\\\w_2\\\\\\vdots\\\\w_N\\end{bmatrix} = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_N\\end{bmatrix}$\n",
    "\n",
    "    $\\implies \\phi w = y $\n",
    "    \n",
    "    $ \\text{If $\\phi$ is invertible, } w = \\phi^{-1}y  $\n",
    "    \n",
    "    \n",
    "* **Effect of $\\gamma$ (Gamma)**\n",
    "\n",
    "    * If gamma is small, each point have wider effect, in comparision to larger gamma.\n",
    "    * <img src=\"data:image/jpeg;base64,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\n",
    "\">\n",
    "\n",
    "\n",
    "* **RBF for classification**\n",
    "    \n",
    "    $ h(x) = sign\\bigg( \\sum_{n=1}^N w_n exp\\big(-\\gamma ||x-x_n||^2)\\bigg) $\n",
    "    \n",
    "    $ h(x) = sign( s ) $\n",
    "    \n",
    "    We will try to make s (signal) as -1 or 1.\n",
    "    \n",
    "    We will try to minimize $ (s - y)^2 \\text{ , on D where y = $\\pm$ 1} $\n",
    "    \n",
    "    \n",
    "* **Relationship to nearest-neighbor method**\n",
    "  \n",
    "  * We adopt y value on basis of k nearest point. (All point have same effect.)\n",
    "  \n",
    "  \n",
    "  * In RBF, we adopt y value on basis of all points. (Nearest point will have more effect.)\n",
    "  \n",
    "\n",
    "  * RBF with K centers, Use K << N centers: $ \\mu_1, ..., \\mu_K instead of x_1, .. x_N $ \n",
    "  \n",
    "      $ h(x) = \\sum_{k=1}^K w_k exp\\big(-\\gamma ||x-\\mu_k||^2\\big) $\n",
    "      \n",
    " \n",
    "  * How to choose the centers $\\mu_k$? K-mean clustering (Lloyd's algorithm)\n",
    "      \n",
    "* **Relationship to Neural Network**\n",
    "    \n",
    "  *  RBF network is very same as neural network apart from features extracted in first layer.\n",
    "  \n",
    "  \n",
    "  * In RBF network, features are extracted as $ |x-\\mu_1|, ...|x-\\mu_k|$ , to get non-linearity we use RBF basis function.\n",
    "  \n",
    "\n",
    "  * However in NN, features are decided by weights in first layer.\n",
    "  \n",
    "* **Choosing $\\gamma$ (gamma)**\n",
    "\n",
    "    Treating $\\gamma$ as a parameter to be learned.\n",
    "    \n",
    "    Iterative approach (Expectation Maximization algorithm which is mixture of Gaussians)\n",
    "    \n",
    "    **Algorithm**\n",
    "    1. Fix $\\gamma$, solve for $w_1,...,w_k$.\n",
    "    2. Fix $w_1,...,w_K$, minimize error wrt $\\gamma$.\n",
    "    3. Go to step 1.\n",
    "   \n",
    "       Also we can have a different $\\gamma_k$ for each center $\\mu_k$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
