{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Derivation\n",
    "\n",
    "Let best fit line be : $ \\hat Y_i = b X_i + a $ and the sum of squared error(S) is $ S = \\sum ( Y_i - \\hat Y_i )^2 $ which is to be minimized.\n",
    "\n",
    "S is to be minimized at the value of a and b, So $ \\partial S / \\partial a = 0 $ and $ \\partial S / \\partial b $\n",
    "\n",
    "So, First Condition\n",
    "\n",
    "$$ \\frac{\\partial S}{ \\partial a} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial a}( \\sum_{i=0}^n ( Y_i - \\hat Y_i )^2 ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial a} \\sum_{i=0}^n ( Y_i - b X_i - a )^2 ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=0}^n -2( Y_i - b X_i - a ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 2 ( na - \\sum_{i=0}^n Y_i + b \\sum_{i=0}^n X_i ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a = \\frac{ \\sum_{i=0}^n Y_i - b \\sum_{i=0}^n X_i }{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a = \\bar Y - b \\bar X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means constant a (the y-intercept) is set such that the line must go through the mean of x and y. Make sense because this point is the \"center\" of the data cloud\n",
    "\n",
    "Now, second Condition\n",
    "\n",
    "$$ \\frac{\\partial S}{ \\partial a} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial b}( \\sum_{i=0}^n ( Y_i - \\hat Y_i )^2 ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial b} (\\sum_{i=0}^n ( Y_i - b X_i - a )^2 ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \\sum_{i=0}^n -2X_i( Y_i - b X_i - a ) = \\sum_{i=0}^n -2( X_iY_i - b X_i^2 - aX_i ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "substituting value of a,\n",
    "\n",
    "$$  \\sum_{i=0}^n ( X_iY_i - b X_i^2 -  X_i \\bar Y + b  X_i \\bar X ) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$   \\sum_{i=0}^n (X_iY_i -  X_i \\bar Y) - b \\sum_{i=0}^n ( X_i^2 - X_i \\bar X) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$   b = \\frac{\\sum_{i=0}^n (X_iY_i -  X_i \\bar Y)}{\\sum_{i=0}^n ( X_i^2 - X_i \\bar X)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also note that,\n",
    "\n",
    "$$ \\sum_{i=0}^n ( \\bar X ^2 - X_i \\bar X ) = 0, \\; and \\sum_{i=0}^n( \\bar X \\bar Y - Y_i \\bar X ) = 0 $$\n",
    "\n",
    "Using this, b can also be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = \\frac{\\sum_{i=0}^n ( X_iY_i -  X_i \\bar Y) + \\sum_{i=0}^n( \\bar X \\bar Y - Y_i \\bar X ) }{\\sum_{i=0}^n ( X_i^2 - X_i \\bar X) + \\sum_{i=0}^n ( \\bar X ^2 - X_i \\bar X ) }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = \\frac{ \\sum_{i=0}^n \\bigg( X_i (Y_i -\\bar Y) - \\bar X ( Y_i - \\bar Y ) \\bigg) }{ \\sum_{i=0}^n \\bigg( X_i^2 - 2 X_i \\bar X +  \\bar X ^2 \\bigg) }   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = \\frac{ \\frac{1}{n} \\sum_{i=0}^n ( X_i - \\bar X )(Y_i - \\bar Y)  }{ \\frac{1}{n} \\sum_{i=0}^n (X_i - \\bar X)^2 }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = \\frac{covariance( X_i,Y_i )}{variance(X_i)}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = \\frac{ r \\sigma_x \\sigma_y }{ \\sigma_x^2 }, where \\; r = pearson's \\; r $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = \\frac{ r*\\sigma_y }{\\sigma_x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For multiple independent variables\n",
    "\n",
    "$$ a = \\bar Y - \\sum b \\bar X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b = (X^TX)^{-1} X^T Y $$\n",
    "\n",
    "> Calculating $ (X^TX)^{-1} $ is O(n^3). So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\n",
    "\n",
    "> Non invertibility of $X^TX$.\n",
    "\n",
    "> 1. Redundant Features. If two features are linearly dependent.\n",
    "\n",
    "> 2. Too many features. E.g. m<n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math  Reading  Writing\n",
       "0    48       68       63\n",
       "1    62       81       72\n",
       "2    79       80       78\n",
       "3    76       83       79\n",
       "4    59       64       62"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('student.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.values[:,[0,1]]\n",
    "y = data.values[:,-1]\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.44746983, 13.33854736]), 68.616)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =  np.dot( np.linalg.inv(np.dot(X.T, X)), np.dot( X.T, y ))\n",
    "a = y.mean() - np.sum(b * X.mean(axis=0))\n",
    "b,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.dot(X,b)+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9098901726717316"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rsquare(y_true,y_pred):\n",
    "    return 1-( np.sum((y_true - y_pred)**2)/ np.sum( (y_true-y_true.mean())**2 ) )\n",
    "rsquare( y, y_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Relationship\n",
    "**Description :** Linear regression needs the relationship between the independent and dependent variables to be linear. \n",
    "\n",
    "**Why? :** Linear regression can learn non-linear relationship\n",
    "\n",
    "**How to check :** The linearity assumption can best be tested with scatter plots.\n",
    "\n",
    "**How to fix :** Try non-linear models like Decision Trees, ANN etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normality\n",
    "**Description :** The linear regression analysis requires all variables to be multivariate normal.\n",
    "\n",
    "**Why? :** \n",
    "\n",
    "**How to check :** Can be checked using histogram or q-q plot\n",
    "\n",
    "**How to fix :** If data is not normal, a non-linear transformation like log transformation might fix this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No or little multicollinearity\n",
    "\n",
    "**Description :** Multicollinearity occurs when the independent variables are too highly correlated with each other.\n",
    "\n",
    "**Why? :**  It undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant. In simple words, coefficient will be biased.\n",
    "\n",
    "**How to check :** Correlation-matrix or variance-inflation factor (VIF)\n",
    "\n",
    "**How to fix :** The simplest way to address the problem is to remove independent variables with high VIF values.\n",
    "\n",
    "\n",
    "**Variance Inflation Factor :**\n",
    "VIF of a independent variable is computed used R-squared statistic of the regression where that independent variable is predicted by all the other independent variables\n",
    "\n",
    "$$ \\text{VIF} = \\frac{1}{1-R^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Auto-Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description :**  linear regression analysis requires that there is little or no autocorrelation in the data.  Autocorrelation occurs when the residuals are not independent from each other.  In other words when the value of y(x+1) is not independent from the value of y(x).\n",
    "\n",
    "**Why? :**\n",
    "\n",
    "**How to check :** Scatterplot or Durbin-Watson test.\n",
    "\n",
    "**Durbin-Watson test: ** Durbin-Watsonâ€™s d tests the null hypothesis that the residuals are not linearly auto-correlated.  While d can assume values between 0 and 4, values around 2 indicate no autocorrelation.  As a rule of thumb values of 1.5 < d < 2.5 show that there is no auto-correlation in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity\n",
    "\n",
    "**Description :** It is a fancy word for \"equal varainces\". Variance of dependent variable is same across all ranges of independent variable.\n",
    "\n",
    "**Why?** Heroskedasticity can produce biased and misleading parameter estimates. \n",
    "\n",
    "**How to check :** Scatterplot of residual and independent variable. If there is a heteroscedasticity, there will a cone like scatterplot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
